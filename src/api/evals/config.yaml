# This configuration defines evaluation parameters and settings

evaluation:
  # Global evaluation settings
  model: "gpt-4"
  temperature: 0.1
  max_tokens: 1000
  
  # Scoring thresholds
  pass_threshold: 0.7
  excellence_threshold: 0.9
  
  # Evaluation criteria weights
  criteria_weights:
    accuracy: 0.3
    code_correctness: 0.25
    helpfulness: 0.2
    completeness: 0.15
    security: 0.1

# Test suite configurations
test_suites:
  main_assistant:
    name: "C# AI Buddy Main Assistant"
    description: "Evaluates the main AI assistant system prompt"
    test_cases_file: "test_cases/csharp_ai_buddy_tests.json"
    evaluation_model: "gpt-4"
    
  categorization:
    name: "AI Framework Categorization"
    description: "Evaluates AI framework categorization accuracy"
    test_cases_file: "test_cases/categorization_tests.json"
    evaluation_model: "gpt-4"

# Reporting configuration
reporting:
  output_formats: ["json", "markdown", "csv"]
  include_detailed_results: true
  include_failure_analysis: true
  score_visualization: true

# CI/CD Integration settings
automation:
  run_on_prompt_changes: true
  fail_build_on_threshold: 0.6
  generate_pr_comments: true
  slack_notifications: false
  
# OpenAI Evals API settings (when using hosted evals)
openai_evals:
  use_hosted_api: true
  eval_name_prefix: "csharp_ai_buddy"
  dataset_upload: true
  track_experiments: true